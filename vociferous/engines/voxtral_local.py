from __future__ import annotations

from pathlib import Path
from typing import Iterable

from vociferous.domain.model import (
    AudioChunk,
    DEFAULT_MODEL_CACHE_DIR,
    EngineConfig,
    EngineMetadata,
    TranscriptSegment,
    TranscriptionEngine,
    TranscriptionOptions,
)
from vociferous.domain.exceptions import DependencyError, EngineError
from vociferous.engines.model_registry import normalize_model_name
from vociferous.engines.hardware import get_optimal_device, get_optimal_compute_type


class VoxtralLocalEngine(TranscriptionEngine):
    """
    Transformers-based local Voxtral engine for offline audio transcription.

    This is a fallback engine for environments without vLLM server access.
    Runs entirely locally using the transformers library with direct GPU/CPU
    inference. Slower than VoxtralVLLMEngine but doesn't require external services.

    For production use with better performance and GPU efficiency, prefer
    VoxtralVLLMEngine (voxtral_vllm) which delegates to a vLLM server.
    """

    def __init__(self, config: EngineConfig) -> None:
        self.config = config
        self.model_name = normalize_model_name("voxtral_local", config.model_name)
        # Hardware-aware defaults
        self.device = config.device if config.device != "auto" else get_optimal_device()
        self.precision = config.compute_type if config.compute_type != "auto" else get_optimal_compute_type(self.device)
        cache_root = Path(config.model_cache_dir or DEFAULT_MODEL_CACHE_DIR).expanduser()
        cache_root.mkdir(parents=True, exist_ok=True)
        self.cache_dir = cache_root
        self._model = None
        self._processor = None

    def _lazy_model(self):
        if self._model is not None:
            return
        try:
            from transformers import VoxtralForConditionalGeneration, AutoProcessor
            import torch
        except ImportError as exc:  # pragma: no cover - optional dependency guard
            raise RuntimeError(
                "transformers and torch are required for VoxtralEngine; install with voxtral extra"
            ) from exc

        if self.device == "cuda" and not torch.cuda.is_available():
            raise EngineError("CUDA requested for Voxtral but no GPU is available")

        self._processor = AutoProcessor.from_pretrained(
            self.model_name, cache_dir=str(self.cache_dir)
        )
        dtype = torch.bfloat16 if self.device == "cuda" else torch.float32
        self._model = VoxtralForConditionalGeneration.from_pretrained(
            self.model_name,
            dtype=dtype,
            cache_dir=str(self.cache_dir),
        ).to(self.device)

    def start(self, options: TranscriptionOptions) -> None:
        self._options = options
        self._buffer = bytearray()
        self._segments = []
        self._lazy_model()

    def push_audio(self, pcm16: bytes, timestamp_ms: int) -> None:
        self._buffer.extend(pcm16)

    def flush(self) -> None:
        if not self._buffer:
            return

        import numpy as np
        import torch

        # Process audio efficiently
        audio_np = np.frombuffer(self._buffer, dtype=np.int16).astype("float32") / 32768.0

        inputs = self._processor.apply_transcription_request(
            audio=[audio_np],
            language=self._options.language or "en",
            model_id=self.model_name,
            sampling_rate=16000,
            format=["wav"],
        ).to(self.device, dtype=self._model.dtype)

        gen_kwargs = {}
        params = self._options.params
        if "max_new_tokens" in params:
            gen_kwargs["max_new_tokens"] = int(params["max_new_tokens"])
        else:
            gen_kwargs["max_new_tokens"] = 2048

        # Note: temperature is not supported for Voxtral transcription mode

        with torch.inference_mode():
            outputs = self._model.generate(**inputs, **gen_kwargs)

        input_length = inputs.input_ids.shape[1]
        new_tokens = outputs[:, input_length:]
        transcription = self._processor.batch_decode(
            new_tokens, skip_special_tokens=True
        )[0]

        if transcription.strip():
            self._segments.append(TranscriptSegment(
                text=transcription.strip(),
                start_s=0.0,
                end_s=len(audio_np) / 16000.0,
                language=self._options.language or "en",
                confidence=1.0
            ))

        self._buffer.clear()

    def poll_segments(self) -> list[TranscriptSegment]:
        segs = list(self._segments)
        self._segments.clear()
        return segs

    @property
    def metadata(self) -> EngineMetadata:
        """Return engine metadata for result building."""
        return EngineMetadata(
            model_name=self.model_name,
            device=self.device,
            precision=self.precision,
        )
