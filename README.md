<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>Vociferous â€” Privacy-First Speech-to-Text for Linux</title>
</head>
<body>

<div align="center">
    <h1>Vociferous</h1>
    <p><strong>Privacy-First Speech-to-Text for Linux</strong></p>
    <p>Your voice. Your machine. Your data.</p>

# Vociferous

### Privacy-First Speech-to-Text for Linux

*Your voice. Your machine. Your data.*

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![PyQt6](https://img.shields.io/badge/GUI-PyQt6-green.svg)](https://www.riverbankcomputing.com/software/pyqt/)
[![Whisper](https://img.shields.io/badge/ASR-OpenAI%20Whisper-orange.svg)](https://github.com/openai/whisper)

<img src="docs/images/transcribe_view.png" width="700" alt="Vociferous Main Interface">

[Features](#features) â€¢ [Installation](#installation) â€¢ [Quick Start](#quick-start) â€¢ [Documentation](#documentation) â€¢ [Architecture](#architecture)

    <p>
        Features â€¢ Installation â€¢ Quick Start â€¢ Documentation â€¢ Architecture
    </p>
</div>

<hr>

## What is Vociferous?

<p>
    Vociferous is a production-grade, local-first dictation system that transforms speech into text entirely on your machine.
    Built with architectural rigor and attention to user experience, it leverages OpenAI's Whisper for state-of-the-art
    transcription and offers optional AI-powered refinement to polish your text with grammar correction and formatting.
</p>

<p>
    Unlike cloud-based alternatives, Vociferous processes everything locallyâ€”your voice never leaves your computer.
    No subscriptions, no usage limits, no privacy compromises.
</p>

<hr>

## Features

<h3>Core Capabilities</h3>
<ul>
    <li>ğŸ”’ <strong>Complete Privacy</strong> â€” All transcription and refinement happens on-device using local models</li>
    <li>ğŸ¯ <strong>Whisper ASR</strong> â€” OpenAI's state-of-the-art speech recognition via faster-whisper</li>
    <li>âœ¨ <strong>AI Refinement</strong> â€” Optional SLM-powered text improvement (grammar, punctuation, formatting)</li>
    <li>ğŸ§ <strong>Native Linux Support</strong> â€” First-class Wayland integration with global hotkey support</li>
    <li>ğŸ“š <strong>Persistent History</strong> â€” SQLite-backed transcript storage with full-text search and organization</li>
    <li>âš¡ <strong>GPU Acceleration</strong> â€” CUDA support for real-time transcription and refinement</li>
    <li>ğŸ¨ <strong>Modern UI</strong> â€” Sleek PyQt6 interface with polished design system</li>
</ul>

- **Complete Privacy** â€” All transcription and refinement happens on-device using local models
- **Whisper ASR** â€” OpenAI's state-of-the-art speech recognition via [faster-whisper](https://github.com/SYSTRAN/faster-whisper)
- **AI Refinement** â€” Optional SLM-powered text improvement (grammar, punctuation, formatting)
- **Native Linux Support** â€” First-class Wayland integration with global hotkey support
- **Persistent History** â€” SQLite-backed transcript storage with full-text search and organization
- **GPU Acceleration** â€” CUDA support for real-time transcription and refinement
- **Modern UI** â€” Sleek PyQt6 interface with polished design system

<hr>

- **Intent-Driven Architecture** â€” Clean separation between user intent and execution logic
- **Dual-Text Model** â€” Preserves raw Whisper output while allowing user edits
- **Pluggable Backends** â€” Modular input handling (evdev/pynput), model selection, and audio processing
- **Production-Ready** â€” Comprehensive test suite, type safety, and architectural guardrails
- **Fully Offline** â€” No internet connection required after initial model download

---

## Screenshots

<details>
<summary><b>View Gallery (Click to expand)</b></summary>

    <table>
        <tr>
            <td align="center">
                <img src="docs/images/transcribe_view.png" width="400" alt="Transcribe View"><br>
                <em>Transcribe View â€” Live dictation and recording</em>
            </td>
            <td align="center">
                <img src="docs/images/history_view.png" width="400" alt="History View"><br>
                <em>History View â€” Browse and manage transcripts</em>
            </td>
        </tr>
        <tr>
            <td align="center">
                <img src="docs/images/search_and_manage_view.png" width="400" alt="Search View"><br>
                <em>Search &amp; Manage â€” Filter and organize</em>
            </td>
            <td align="center">
                <img src="docs/images/refinement_view.png" width="400" alt="Refine View"><br>
                <em>Refine View â€” AI-powered text improvement</em>
            </td>
        </tr>
        <tr>
            <td align="center">
                <img src="docs/images/settings_view.png" width="400" alt="Settings View"><br>
                <em>Settings View â€” Configure transcription and refinement</em>
            </td>
            <td align="center">
                <img src="docs/images/user_view.png" width="400" alt="User View"><br>
                <em>User View â€” Metrics and documentation</em>
            </td>
        </tr>
    </table>

    <h4>Onboarding Experience</h4>

    <table>
        <tr>
            <td align="center">
                <img src="docs/images/onboarding_welcome.png" width="300" alt="Onboarding Welcome"><br>
                <em>Welcome screen</em>
            </td>
            <td align="center">
                <img src="docs/images/onboarding_transcription_model_choice.png" width="300" alt="Model Selection"><br>
                <em>Model selection</em>
            </td>
            <td align="center">
                <img src="docs/images/onboarding_choose_hotkey.png" width="300" alt="Hotkey Setup"><br>
                <em>Hotkey configuration</em>
            </td>
        </tr>
    </table>
</details>

---

## Installation

### Prerequisites

| Requirement | Minimum | Recommended |
|-------------|---------|-------------|
| **OS** | Linux (X11/Wayland) | Linux (Wayland) |
| **Python** | 3.12+ | 3.12 |
| **RAM** | 4 GB | 8 GB |
| **GPU** | None (CPU mode) | NVIDIA CUDA |
| **VRAM** | N/A | 4+ GB (for refinement) |

### Wayland Setup

For global hotkeys on Wayland, add your user to the `input` group:

```bash
sudo usermod -a -G input $USER
# Log out and back in for changes to take effect
```

### Install Steps

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/Vociferous.git
   cd Vociferous
   ```

2. **Create virtual environment**
   ```bash
   python3 -m venv .venv
   ```

3. **Install dependencies**
   ```bash
   .venv/bin/pip install -r requirements.txt
   ```

4. **Launch Vociferous**
   ```bash
   ./vociferous
   ```

> **Important:** Always use the `./vociferous` launcher script. Running `python src/main.py` directly bypasses GPU library configuration.

---

## Quick Start

### Your First Recording

1. **Launch** the application with `./vociferous`
2. **Press Right Alt** (default hotkey) to start recording
3. **Speak clearly** into your microphone
4. **Press Right Alt again** to stop recording
5. **Wait** for Whisper to transcribe your speech
6. **Review** your transcript in the main panel

### Default Configuration

| Setting | Default Value |
|---------|---------------|
| Whisper Model | `distil-large-v3` (~1.5 GB) |
| Device | Auto-detect (GPU if available) |
| Language | English (`en`) |
| Recording Mode | Push-to-talk |
| Hotkey | `Right Alt` |
| Refinement | Disabled (optional) |

### Available Actions

After transcription completes:
- **Copy** â€” Copy text to clipboard
- **Edit** â€” Modify the transcript
- **Delete** â€” Remove the transcript
- **Refine** â€” Polish with AI (if enabled)
- **Save** â€” Persist to history database

---

## Optional AI Refinement

Vociferous includes an optional **text refinement system** powered by local language models.

### What Does Refinement Do?

- Fixes grammar and punctuation errors
- Improves sentence structure and flow
- Applies consistent formatting
- Preserves original intent and meaning

### Enabling Refinement

1. Open **Settings**
2. Toggle **Enable AI Refinement** to ON
3. Select your preferred **SLM Model** (e.g., Qwen3-4B-Instruct)
4. Click **Apply**

On first use, Vociferous will download and convert the model (~4 GB). This happens once per model and takes several minutes.

### GPU Requirements

Refinement models require:
- **CUDA-capable NVIDIA GPU** with 4+ GB VRAM (recommended)
- **CPU fallback** supported (slower, ~8+ GB RAM recommended)

---

## Documentation

Comprehensive documentation is available in the [**project wiki**](docs/wiki):

### Core Concepts
- [**Architecture**](docs/wiki/Architecture.md) â€” System design, threading model, component boundaries
- [**Design System**](docs/wiki/Design-System.md) â€” Colors, typography, spacing tokens
- [**Data & Persistence**](docs/wiki/Data-and-Persistence.md) â€” Database schema, dual-text invariant

### User Guides
- [**Getting Started**](docs/wiki/Getting-Started.md) â€” Installation and first-run guide
- [**UI Views Overview**](docs/wiki/UI-Views-Overview.md) â€” View architecture and capabilities

### View Documentation
- [**Transcribe View**](docs/wiki/View-Transcribe.md) â€” Live recording and dictation
- [**History View**](docs/wiki/View-History.md) â€” Browse and manage past transcripts
- [**Search View**](docs/wiki/View-Search.md) â€” Filter and find transcripts
- [**Refine View**](docs/wiki/View-Refine.md) â€” AI-powered text refinement
- [**Settings View**](docs/wiki/View-Settings.md) â€” Configure application options
- [**User View**](docs/wiki/View-User.md) â€” Metrics, about, and documentation

### Advanced Topics
- [**Refinement System**](docs/wiki/Refinement-System.md) â€” SLM service, model provisioning, prompt engineering
- [**Testing Philosophy**](docs/wiki/Testing-Philosophy.md) â€” Test tiers, fixtures, CI strategy

---

## Architecture

Vociferous is built with **architectural rigor** and follows strict design principles to ensure maintainability and extensibility.

### Intent-Driven Design

All user interactions follow a strict **Intent Pattern**:

```
User Action â†’ Intent (immutable dataclass) â†’ Signal â†’ Controller â†’ Execution
```

This ensures:
- Clean separation between UI and business logic
- Testable and predictable behavior
- No spaghetti code or hidden side effects

### Technology Stack

| Layer | Technology |
|-------|------------|
| Language | Python 3.12+ |
| UI Framework | PyQt6 6.7.0+ |
| Speech Recognition | faster-whisper (CTranslate2) |
| Text Refinement | CTranslate2 + Qwen3-4B-Instruct |
| Database | SQLAlchemy 2.0+ (SQLite) |
| Input Handling | evdev (Wayland) / pynput (X11) |
| GPU Acceleration | CUDA (optional) |

### Component Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    UI Layer (PyQt6)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              Application Coordinator                â”‚
â”‚    (Composition root, signal wiring, lifecycle)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Services Layer                                     â”‚
â”‚  â€¢ TranscriptionService    â€¢ SLMService             â”‚
â”‚  â€¢ AudioService           â€¢ VoiceCalibration        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Core Runtime (Background Engine)                   â”‚
â”‚  â€¢ Whisper Inference      â€¢ Audio Capture           â”‚
â”‚  â€¢ State Management       â€¢ IPC Protocol            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Database Layer (SQLAlchemy + SQLite)               â”‚
â”‚  â€¢ HistoryManager         â€¢ Models & DTOs           â”‚
â”‚  â€¢ Repositories           â€¢ Signal Bridge           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Development

### Requirements

- Python 3.12+
- Virtual environment (`.venv/`)
- Development tools: `ruff`, `mypy`, `pytest`

### Running Tests

```bash
# Full test suite
VOCIFEROUS_TEST_IGNORE_RUNNING=1 ./scripts/check.sh

# Individual test categories
.venv/bin/pytest tests/unit/
.venv/bin/pytest tests/integration/
.venv/bin/pytest tests/contracts/
```

### Code Quality

```bash
# Linting
.venv/bin/ruff check .

# Type checking
.venv/bin/mypy .

# Auto-formatting
.venv/bin/ruff format .
```

### Project Structure

```
vociferous/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/              # Application coordination, config, exceptions
â”‚   â”œâ”€â”€ core_runtime/      # Background engine and IPC
â”‚   â”œâ”€â”€ database/          # SQLAlchemy models and persistence
â”‚   â”œâ”€â”€ services/          # Business logic (transcription, SLM, audio)
â”‚   â”œâ”€â”€ ui/                # PyQt6 views, components, styles
â”‚   â””â”€â”€ input_handler/     # Keyboard/input backends
â”œâ”€â”€ tests/                 # Comprehensive test suite
â”œâ”€â”€ docs/wiki/             # User and developer documentation
â””â”€â”€ assets/                # Icons, sounds, resources
```

---

## Contributing

Vociferous is built with high standards for code quality and architectural integrity. Before contributing:

1. Read the [Architecture documentation](docs/wiki/Architecture.md)
2. Review the [Testing Philosophy](docs/wiki/Testing-Philosophy.md)
3. Ensure all tests pass: `./scripts/check.sh`
4. Follow the intent-driven design pattern
5. Update documentation for any behavioral changes

---

## License

This project is licensed under the **MIT License** â€” see the [LICENSE](LICENSE) file for details.

---

## Acknowledgments

- **[OpenAI Whisper](https://github.com/openai/whisper)** â€” Foundation of the transcription engine
- **[faster-whisper](https://github.com/SYSTRAN/faster-whisper)** â€” CTranslate2-based Whisper inference
- **[PyQt6](https://www.riverbankcomputing.com/software/pyqt/)** â€” Powerful cross-platform GUI framework
- **[SQLAlchemy](https://www.sqlalchemy.org/)** â€” The Python SQL toolkit
- **[Qwen Team](https://huggingface.co/Qwen)** â€” High-quality open-source language models

---

<div align="center">

**Built with love for the Linux community**

[Back to Top](#vociferous)

</div>

</body>
</html>