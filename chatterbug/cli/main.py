from __future__ import annotations

from pathlib import Path
import logging
import sys

from chatterbug.app import TranscriptionSession, configure_logging
from chatterbug.audio.sources import FileSource, MicrophoneSource
from chatterbug.cli.sinks import ClipboardSink, FileSink, HistorySink, StdoutSink, CompositeSink
from chatterbug.config import load_config
from chatterbug.domain import EngineConfig, TranscriptionOptions
from chatterbug.domain.exceptions import (
    DependencyError, EngineError, AudioDecodeError, ConfigurationError
)
from chatterbug.engines.factory import build_engine
from chatterbug.engines.model_registry import normalize_model_name
from chatterbug.polish.base import PolisherConfig
from chatterbug.polish.factory import build_polisher
from chatterbug.tui import run_tui
from chatterbug.storage.history import HistoryStorage

try:
    import typer
except ImportError as exc:  # pragma: no cover - tooling dependency guard
    raise DependencyError("typer is required for the CLI") from exc


configure_logging()

app = typer.Typer(help="ChatterBug CLI")
cli_app = app  # alias for embedding


@app.command()
def transcribe(
    file: Path,
    engine: str = "whisper_turbo",
    language: str = "en",
    output: Path | None = typer.Option(None, help="Path to write transcript (defaults to stdout)"),
    model: str | None = typer.Option(None, help="Model name to load (engine-specific)"),
    device: str | None = typer.Option(None, help="Device to use (cpu, cuda); defaults to config (cpu by default)"),
    compute_type: str | None = typer.Option(None, help="Compute type (int8, int8_float16, float16)"),
    numexpr_max_threads: int | None = typer.Option(None, help="Limit NumExpr threads (set env NUMEXPR_MAX_THREADS)"),
    word_timestamps: bool = typer.Option(False, help="Enable word-level timestamps for Whisper"),
    enable_batching: bool = typer.Option(True, help="Enable batched inference for Whisper (default on for files)"),
    batch_size: int = typer.Option(16, help="Batch size when batching is enabled for Whisper"),
    beam_size: int = typer.Option(5, help="Beam size for Whisper decoding (None to use library default)", show_default=True),
    vad_filter: bool = typer.Option(True, help="Apply VAD filtering before transcription"),
    chunk_ms: int = typer.Option(30000, help="Chunk size for file source (ms); use larger for better context"),
    trim_tail_ms: int = typer.Option(800, help="Trim trailing silence window (ms)"),
    noise_gate_db: float | None = typer.Option(None, help="Noise gate threshold in dBFS (e.g., -40). None disables."),
    whisper_temperature: float = typer.Option(0.0, help="Whisper decoding temperature (0 uses library default)"),
    preset: str | None = typer.Option(
        None,
        help="Preset for speed/quality tradeoff",
        case_sensitive=False,
        rich_help_panel="Presets",
        show_default=False,
    ),
    prompt: str | None = typer.Option(None, help="Prompt/task (used by Voxtral smart mode)"),
    max_new_tokens: int = typer.Option(0, help="Voxtral: max_new_tokens generation limit (0=unset)"),
    gen_temperature: float = typer.Option(0.0, help="Voxtral: temperature (0 uses model default)"),
    clean_disfluencies: bool = typer.Option(False, help="Remove simple disfluencies (stutters/hyphens) in Whisper output"),
    no_clean_disfluencies: bool = typer.Option(False, help="Disable disfluency cleaning (enabled by default)"),
    fast: bool = typer.Option(False, help="Enable fast mode: batching, distil-large-v3 for English, beam_size=1, optimized settings"),
    polish: bool | None = typer.Option(
        None,
        help="Enable transcript polishing (local heuristic by default)",
        show_default=False,
    ),
    polish_model: str | None = typer.Option(None, help="Optional polisher model name"),
    polish_max_tokens: int = typer.Option(128, help="Max tokens generated by polisher"),
    polish_temperature: float = typer.Option(0.2, help="Sampling temperature for polisher"),
    polish_gpu_layers: int = typer.Option(0, help="GPU layers for llama.cpp polisher (0=CPU)"),
    polish_context_length: int = typer.Option(2048, help="Context length for polisher"),
    clipboard: bool = typer.Option(False, help="Copy final transcript to clipboard"),
    save_history: bool = typer.Option(False, help="Persist transcript to history"),
) -> None:
    """Transcribe a single file using the selected engine and sinks."""
    logging.basicConfig(level=logging.INFO)
    config = load_config()
    # Apply preset overrides
    preset_lower = (preset or "").lower()
    if preset_lower == "fast-gpu":
        compute_type = compute_type or "float16"
        batch_size = max(batch_size, 16)
        beam_size = min(beam_size, 3)
        vad_filter = True
    elif preset_lower == "cpu-balanced":
        compute_type = compute_type or "int8"
        batch_size = max(batch_size, 8)
        beam_size = max(beam_size, 5)
        vad_filter = True

    # Apply --fast mode overrides
    if fast:
        enable_batching = True
        batch_size = max(batch_size, 16)
        beam_size = 1
        # Use distil-large-v3 for English unless model is explicitly specified
        if model is None and language == "en" and engine == "whisper_turbo":
            model = "distil-whisper/distil-large-v3"
        # Don't enable word_timestamps in fast mode unless explicitly requested
        # (word_timestamps stays False by default)

    if numexpr_max_threads is None:
        env_threads = config.numexpr_max_threads
    else:
        env_threads = numexpr_max_threads
    if env_threads is not None:
        import os
        os.environ["NUMEXPR_MAX_THREADS"] = str(env_threads)

    polish_enabled = config.polish_enabled if polish is None else polish
    polisher_config = PolisherConfig(
        enabled=polish_enabled,
        model=polish_model or config.polish_model,
        params={
            **config.polish_params,
            "max_tokens": str(polish_max_tokens),
            "temperature": str(polish_temperature),
            "gpu_layers": str(polish_gpu_layers),
            "context_length": str(polish_context_length),
        },
    )

    engine_config = EngineConfig(
        model_name=normalize_model_name(engine, model or config.model_name if engine == config.engine else model),
        compute_type=compute_type or config.compute_type,
        device=device or config.device,
        model_cache_dir=config.model_cache_dir,
        params={
            **config.params,
            "word_timestamps": str(word_timestamps).lower(),
            "enable_batching": str(enable_batching).lower(),
            "batch_size": str(batch_size),
            "vad_filter": str(vad_filter).lower(),
            # Default is true in engine; flags override: explicit enable OR (no explicit disable)
            "clean_disfluencies": str(clean_disfluencies or not no_clean_disfluencies).lower(),
        },
    )
    options = TranscriptionOptions(
        language=language,
        prompt=prompt,
        params={
            "max_new_tokens": str(max_new_tokens) if max_new_tokens > 0 else "",
            "temperature": str(gen_temperature) if gen_temperature > 0 else "",
        },
        beam_size=beam_size if beam_size > 0 else None,
        temperature=whisper_temperature if whisper_temperature > 0 else None,
    )
    
    try:
        engine_adapter = build_engine(engine, engine_config)
        polisher = build_polisher(polisher_config)
    except (DependencyError, EngineError) as exc:
        typer.echo(f"Engine initialization error: {exc}", err=True)
        raise typer.Exit(code=3) from exc
    except ConfigurationError as exc:
        typer.echo(f"Polisher error: {exc}", err=True)
        raise typer.Exit(code=2) from exc
    
    source = FileSource(
        file,
        chunk_ms=chunk_ms,
        trim_tail_ms=trim_tail_ms,
        noise_gate_db=noise_gate_db,
    )
    sinks: list = []
    if output:
        sinks.append(FileSink(output))
    if clipboard:
        sinks.append(ClipboardSink())
    if save_history:
        storage = HistoryStorage(Path(config.history_dir), limit=config.history_limit)
        sinks.append(HistorySink(storage, target=output))
    if not sinks:
        sinks.append(StdoutSink())
    sink = CompositeSink(sinks)
    session = TranscriptionSession()

    try:
        session.start(source, engine_adapter, sink, options, engine_kind=engine, polisher=polisher)
        session.join()
    except FileNotFoundError as exc:
        typer.echo(f"Error: {exc}", err=True)
        raise typer.Exit(code=2) from exc
    except EngineError as exc:
        typer.echo(f"Inference error: {exc}", err=True)
        raise typer.Exit(code=4) from exc
    except (AudioDecodeError, ConfigurationError) as exc:
        typer.echo(f"Decode/config error: {exc}", err=True)
        raise typer.Exit(code=2) from exc
    except Exception as exc:  # pragma: no cover - safety net
        typer.echo(f"Unexpected error: {exc}", err=True)
        raise typer.Exit(code=1) from exc


@app.command()
def listen(
    engine: str = "whisper_turbo",
    language: str = "en",
    device: str | None = typer.Option(None, help="Device to use (cpu, cuda); defaults to config (cpu by default)"),
    numexpr_max_threads: int | None = typer.Option(None, help="Limit NumExpr threads (set env NUMEXPR_MAX_THREADS)"),
    polish: bool = typer.Option(False, help="Enable transcript polishing (local heuristic)"),
    polish_model: str | None = typer.Option(None, help="Optional polisher model name"),
    polish_max_tokens: int = typer.Option(128, help="Max tokens generated by polisher"),
    polish_temperature: float = typer.Option(0.2, help="Sampling temperature for polisher"),
    polish_gpu_layers: int = typer.Option(0, help="GPU layers for llama.cpp polisher (0=CPU)"),
    polish_context_length: int = typer.Option(2048, help="Context length for polisher"),
) -> None:
    """Start live microphone transcription and stream segments to stdout."""
    config = load_config()
    polisher_config = PolisherConfig(
        enabled=polish or config.polish_enabled,
        model=polish_model or config.polish_model,
        params={
            **config.polish_params,
            "max_tokens": str(polish_max_tokens),
            "temperature": str(polish_temperature),
            "gpu_layers": str(polish_gpu_layers),
            "context_length": str(polish_context_length),
        },
    )
    if numexpr_max_threads is None:
        env_threads = config.numexpr_max_threads
    else:
        env_threads = numexpr_max_threads
    if env_threads is not None:
        import os
        os.environ["NUMEXPR_MAX_THREADS"] = str(env_threads)
    try:
        polisher = build_polisher(polisher_config)
    except ValueError as exc:
        typer.echo(f"Polisher error: {exc}", err=True)
        raise typer.Exit(code=2) from exc
    engine_config = EngineConfig(
        model_name=config.model_name,
        compute_type=config.compute_type,
        device=device or config.device,
        model_cache_dir=config.model_cache_dir,
        params=config.params,
    )
    options = TranscriptionOptions(language=language)
    engine_adapter = build_engine(engine, engine_config)
    source = MicrophoneSource()
    sink = StdoutSink()
    session = TranscriptionSession()
    typer.echo("Press Ctrl+C to stop.")
    try:
        session.start(source, engine_adapter, sink, options, engine_kind=engine, polisher=polisher)
        while True:
            import time

            time.sleep(0.1)
    except KeyboardInterrupt:
        session.stop()
        session.join()
    except Exception as exc:  # pragma: no cover - safety net
        session.stop()
        typer.echo(f"Unexpected error: {exc}", err=True)
        raise typer.Exit(code=1) from exc


@app.command()
def tui(
    file: Path | None = typer.Option(None, help="Optional audio file; default is microphone"),
    engine: str = "whisper_turbo",
    language: str = "en",
) -> None:
    """Run minimal TUI that streams transcripts live."""
    try:
        run_tui(file=file, engine=engine, language=language)
    except Exception as exc:
        typer.echo(f"TUI error: {exc}", err=True)
        raise typer.Exit(code=1) from exc


@app.command()
def check() -> None:
    """Verify local prerequisites (ffmpeg, sounddevice, model cache path)."""
    import importlib.util
    import shutil

    ok = True
    if shutil.which("ffmpeg") is None:
        typer.echo("ffmpeg not found on PATH", err=True)
        ok = False
    if importlib.util.find_spec("sounddevice") is None:
        typer.echo("sounddevice not installed (required for microphone capture)", err=True)
        ok = False
    cfg = load_config()
    typer.echo(f"Model cache: {cfg.model_cache_dir}")
    if not ok:
        raise typer.Exit(code=1)
    typer.echo("Environment looks OK.")


def main() -> None:
    """CLI entrypoint for ChatterBug."""
    try:
        app()
    except KeyboardInterrupt:
        typer.echo("\nInterrupted by user.", err=True)
        raise typer.Exit(code=130)
    except Exception as exc:
        typer.echo(f"Fatal error: {exc}", err=True)
        raise typer.Exit(code=1) from exc


if __name__ == "__main__":
    main()
